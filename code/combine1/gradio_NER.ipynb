{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN1sdIz4e2r+0ZbQaOGHpED"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**mounted drive**"],"metadata":{"id":"_kXp-1Y9DHlf"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0HC6zuX3C7Jx","executionInfo":{"status":"ok","timestamp":1693845328483,"user_tz":-420,"elapsed":22276,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"00688950422503571962"}},"outputId":"42cbefa1-bd65-4616-fec4-b9f0bddd1ff6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["**Import lib**"],"metadata":{"id":"edJKh3beDJhG"}},{"cell_type":"code","source":["!pip install tensorflow_addons\n","!pip install gensim==4.1.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NOUcBQy8DKng","executionInfo":{"status":"ok","timestamp":1693845420391,"user_tz":-420,"elapsed":55055,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"00688950422503571962"}},"outputId":"becd9fc1-3dd8-405f-d70f-18d359910590"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow_addons\n","  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/612.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/612.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m604.2/612.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.1)\n","Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Installing collected packages: typeguard, tensorflow_addons\n","Successfully installed tensorflow_addons-0.21.0 typeguard-2.13.3\n","Collecting gensim==4.1.2\n","  Downloading gensim-4.1.2.tar.gz (23.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from gensim==4.1.2) (1.23.5)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.1.2) (1.10.1)\n","Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.1.2) (6.3.0)\n","Building wheels for collected packages: gensim\n","  Building wheel for gensim (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gensim: filename=gensim-4.1.2-cp310-cp310-linux_x86_64.whl size=25996764 sha256=1162edaf23856c22acfea907e0090963869bbd2013ee0515712cdc068019f11e\n","  Stored in directory: /root/.cache/pip/wheels/13/35/4e/dca2954de21981d0a137ff930239f0767403a617e32f19f04f\n","Successfully built gensim\n","Installing collected packages: gensim\n","  Attempting uninstall: gensim\n","    Found existing installation: gensim 4.3.1\n","    Uninstalling gensim-4.3.1:\n","      Successfully uninstalled gensim-4.3.1\n","Successfully installed gensim-4.1.2\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5a6JoARDChzE","executionInfo":{"status":"ok","timestamp":1693845430219,"user_tz":-420,"elapsed":5131,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"00688950422503571962"}},"outputId":"008c1ff6-1b9f-4073-9a03-93c77ccbd394"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","import tensorflow.keras.layers as L\n","from tensorflow_addons.text import crf_log_likelihood, crf_decode\n","from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed, Input, BatchNormalization, Dropout\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras import initializers, Sequential\n","import tensorflow.keras.optimizers as Optimizer"]},{"cell_type":"markdown","source":["**Set up model**"],"metadata":{"id":"mFoPlsW1DZZn"}},{"cell_type":"code","source":["class CRF(L.Layer):\n","    def __init__(self,\n","                 output_dim,\n","                 sparse_target=True,\n","                 **kwargs):\n","        \"\"\"\n","        Args:\n","            output_dim (int): the number of labels to tag each temporal input.\n","            sparse_target (bool): whether the the ground-truth label represented in one-hot.\n","        Input shape:\n","            (batch_size, sentence length, output_dim)\n","        Output shape:\n","            (batch_size, sentence length, output_dim)\n","        \"\"\"\n","        super(CRF, self).__init__(**kwargs)\n","        self.output_dim = int(output_dim)\n","        self.sparse_target = sparse_target\n","        self.input_spec = L.InputSpec(min_ndim=3)\n","        self.supports_masking = False\n","        self.sequence_lengths = None\n","        self.transitions = None\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 3\n","        f_shape = tf.TensorShape(input_shape)\n","        input_spec = L.InputSpec(min_ndim=3, axes={-1: f_shape[-1]})\n","\n","        if f_shape[-1] is None:\n","            raise ValueError('The last dimension of the inputs to `CRF` '\n","                             'should be defined. Found `None`.')\n","        if f_shape[-1] != self.output_dim:\n","            raise ValueError('The last dimension of the input shape must be equal to output'\n","                             ' shape. Use a linear layer if needed.')\n","        self.input_spec = input_spec\n","        self.transitions = self.add_weight(name='transitions',\n","                                           shape=[self.output_dim, self.output_dim],\n","                                           initializer='glorot_uniform',\n","                                           trainable=True)\n","        self.built = True\n","\n","    def compute_mask(self, inputs, mask=None):\n","        # Just pass the received mask from previous layer, to the next layer or\n","        # manipulate it if this layer changes the shape of the input\n","        return mask\n","\n","    def call(self, inputs, sequence_lengths=None, training=None, **kwargs):\n","        sequences = tf.convert_to_tensor(inputs, dtype=self.dtype)\n","        if sequence_lengths is not None:\n","            assert len(sequence_lengths.shape) == 2\n","            assert tf.convert_to_tensor(sequence_lengths).dtype == 'int32'\n","            seq_len_shape = tf.convert_to_tensor(sequence_lengths).get_shape().as_list()\n","            assert seq_len_shape[1] == 1\n","            self.sequence_lengths = K.flatten(sequence_lengths)\n","        else:\n","            self.sequence_lengths = tf.ones(tf.shape(inputs)[0], dtype=tf.int32) * (\n","                tf.shape(inputs)[1]\n","            )\n","\n","        viterbi_sequence, _ = crf_decode(sequences,\n","                                         self.transitions,\n","                                         self.sequence_lengths)\n","        output = K.one_hot(viterbi_sequence, self.output_dim)\n","        return K.in_train_phase(sequences, output)\n","\n","    @property\n","    def loss(self):\n","        def crf_loss(y_true, y_pred):\n","            y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype)\n","            log_likelihood, self.transitions = crf_log_likelihood(\n","                y_pred,\n","                tf.cast(K.argmax(y_true), dtype=tf.int32) if self.sparse_target else y_true,\n","                self.sequence_lengths,\n","                transition_params=self.transitions,\n","            )\n","            return tf.reduce_mean(-log_likelihood)\n","        return crf_loss\n","\n","    @property\n","    def accuracy(self):\n","        def viterbi_accuracy(y_true, y_pred):\n","            # -1e10 to avoid zero at sum(mask)\n","            mask = K.cast(\n","                K.all(K.greater(y_pred, -1e10), axis=2), K.floatx())\n","            shape = tf.shape(y_pred)\n","            sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n","            y_pred, _ = crf_decode(y_pred, self.transitions, sequence_lengths)\n","            if self.sparse_target:\n","                y_true = K.argmax(y_true, 2)\n","            y_pred = K.cast(y_pred, 'int32')\n","            y_true = K.cast(y_true, 'int32')\n","            corrects = K.cast(K.equal(y_true, y_pred), K.floatx())\n","            return K.sum(corrects * mask) / K.sum(mask)\n","        return viterbi_accuracy\n","\n","    def compute_output_shape(self, input_shape):\n","        tf.TensorShape(input_shape).assert_has_rank(3)\n","        return input_shape[:2] + (self.output_dim,)\n","\n","    def get_config(self):\n","        config = {\n","            'output_dim': self.output_dim,\n","            'sparse_target': self.sparse_target,\n","            'supports_masking': self.supports_masking,\n","            'transitions': K.eval(self.transitions)\n","        }\n","        base_config = super(CRF, self).get_config()\n","        return dict(base_config, **config)"],"metadata":{"id":"FQAdXx4ZCmja","executionInfo":{"status":"ok","timestamp":1693845435350,"user_tz":-420,"elapsed":364,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"00688950422503571962"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["**Word tokenize module**"],"metadata":{"id":"I_CGQ732Dj0C"}},{"cell_type":"code","source":["!pip install py_vncorenlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uK-_PvADDldc","executionInfo":{"status":"ok","timestamp":1693845482518,"user_tz":-420,"elapsed":5814,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"00688950422503571962"}},"outputId":"5325e8e8-cebb-440b-a8d8-536e51145f64"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting py_vncorenlp\n","  Downloading py_vncorenlp-0.1.4.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pyjnius (from py_vncorenlp)\n","  Downloading pyjnius-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: py_vncorenlp\n","  Building wheel for py_vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for py_vncorenlp: filename=py_vncorenlp-0.1.4-py3-none-any.whl size=4306 sha256=1fa23dda20a36e3a5bfc1a45ff79b81579d9f4e084207f43ed31568e83164636\n","  Stored in directory: /root/.cache/pip/wheels/d5/d9/bf/62632cdb007c702a0664091e92a0bb1f18a2fcecbe962d9827\n","Successfully built py_vncorenlp\n","Installing collected packages: pyjnius, py_vncorenlp\n","Successfully installed py_vncorenlp-0.1.4 pyjnius-1.5.0\n"]}]},{"cell_type":"code","source":["!rm -r /content/models"],"metadata":{"id":"6mKf6YzFEc-0","executionInfo":{"status":"ok","timestamp":1693845680192,"user_tz":-420,"elapsed":391,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"00688950422503571962"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import py_vncorenlp\n","\n","py_vncorenlp.download_model(save_dir='/content/')"],"metadata":{"id":"KcUu1yu_DnkE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir='/content/')"],"metadata":{"id":"6nL28U13E7hU","executionInfo":{"status":"ok","timestamp":1693845799363,"user_tz":-420,"elapsed":902,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"00688950422503571962"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["**get tokenize**"],"metadata":{"id":"JSF9sr0YDtrB"}},{"cell_type":"code","source":["from gensim.models import FastText\n","import pickle\n","trained = True\n","\n","model_fasttext = FastText.load('/content/drive/MyDrive/CRF/Data/Fasttext/model_fasttext_gensim.bin')\n","\n","with open('/content/drive/MyDrive/CRF/tokenizer.pickle', 'rb') as handle:\n","    tokenizer = pickle.load(handle)\n","\n","word_index = tokenizer.word_index\n","emb_mean, emb_std = -0.5,0.5\n","embed_size = 100 #Kích thước vector biểu diễn 1 từ\n","nb_words = len(word_index) + 1\n","embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n","for word, i in word_index.items():\n","    if i >= nb_words:\n","        continue\n","    if word in model_fasttext.wv.key_to_index:\n","        embedding_matrix[i] = model_fasttext.wv.get_vector(word)\n","\n","with open('/content/drive/MyDrive/CRF/tag_tokenizer.pickle', 'rb') as handle:\n","    tag_tokenizer = pickle.load(handle)\n","\n","tag_index = tag_tokenizer.word_index\n","tag_size = len(tag_index) + 1"],"metadata":{"id":"fqKrIGpGCxZl","executionInfo":{"status":"ok","timestamp":1693845823317,"user_tz":-420,"elapsed":18717,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"00688950422503571962"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def create_model(embeddings_matrix, vocab_size, embedding_dim, max_length):\n","    crf = CRF(len(tag_index), sparse_target=True)\n","    input = Input(shape = (max_length, ), dtype='int32', name='input_text')\n","    x = Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n","                  weights=[embedding_matrix])(input)\n","    x = Bidirectional(LSTM(units=max_length, return_sequences=True,\n","                                recurrent_dropout=0.01))(x)\n","    x = TimeDistributed(Dense(128, activation='relu', kernel_initializer='he_normal'))(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(rate=0.6)(x)\n","    x = Dense(len(tag_index), activation='relu', kernel_initializer='he_normal')(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(rate=0.1)(x)\n","    output = crf(x)\n","    model_final = Model(input, output)\n","    model_final.compile(optimizer=Optimizer.Adam(lr=0.001), loss=crf.loss,\n","                        metrics=[crf.accuracy])\n","\n","    return model_final\n","\n","model_ner = create_model(embedding_matrix, nb_words, embed_size, max_length = 100)\n","model_ner.load_weights(\"/content/drive/MyDrive/CRF/Data/Fasttext/best_weight.hdf5\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i7wznfMbC0Ps","executionInfo":{"status":"ok","timestamp":1693845912813,"user_tz":-420,"elapsed":5551,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"00688950422503571962"}},"outputId":"6420ab0c-80e6-41ae-893f-c5b72fbe3adb"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]}]},{"cell_type":"code","source":["def get_tags(sequences, tag_index):\n","    sequence_tags = []\n","    for sequence in sequences:\n","        sequence_tag = []\n","        for categorical in sequence:\n","            sequence_tag.append(tag_index.get(np.argmax(categorical)))\n","        sequence_tags.append(sequence_tag)\n","    return sequence_tags\n","\n","def predict(model, tag_tokenizer, sent):\n","    tag_index = tag_tokenizer.word_index\n","    tag_size = len(tag_index) + 1\n","    pred = model.predict(sent)\n","    sequence_tags = get_tags(pred, {i: t for t, i in tag_index.items()})\n","    for idx, each in enumerate(sequence_tags):\n","        try:\n","           idx_cut = each.index(None)\n","        except:\n","           idx_cut = len(each) + 1\n","        sequence_tags[idx] = each[:idx_cut]\n","    return sequence_tags\n","\n","def match_pair_ner(text, ner):\n","    dict_ = {}\n","    text_arr = text.split(' ')\n","    save_ner, save_words = None, ''\n","    for i in range(len(text_arr)):\n","      if ner[i] == 'O' or 'B_' in ner[i] or ('I_' in ner[i] and ner[i - 1] == 'O'):\n","        if save_ner is not None and save_ner != 'O':\n","          if save_ner not in dict_:\n","            dict_[save_ner] = []\n","          dict_[save_ner].append(save_words.replace('_', ' ').strip())\n","        save_words = text_arr[i] + ' '\n","      else:\n","        save_words += text_arr[i] + ' '\n","      save_ner = ner[i] if ner[i] == 'O' else ner[i][2:]\n","    if save_ner is not None and save_ner != 'O':\n","        if save_ner not in dict_:\n","            dict_[save_ner] = []\n","        dict_[save_ner].append(save_words.replace('_', ' ').strip())\n","    return dict_"],"metadata":{"id":"rNTQ0HvHC4mP","executionInfo":{"status":"ok","timestamp":1693845914958,"user_tz":-420,"elapsed":331,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"00688950422503571962"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["**Infer**"],"metadata":{"id":"xakdoSuyDcfk"}},{"cell_type":"code","source":["month_list = [\"t\",\"th\",\"thg\",\"tháng\"]\n","quy_list = [\"q\",\"quý\"]\n","number_list = [*range(1,13)]\n","num_quarter_list = [*range(1,5)]\n","\n","def infer_ner(text):\n","    text = text.replace('?','').replace('/',' ').replace(\"\\\\\",\" \").replace('.',\" \").replace(\"_\",\"\").lower()\n","    for month_desc in month_list:\n","      for num in number_list:\n","        text = text.replace(month_desc + str(num),month_desc + \" \" + str(num))\n","    for quy in quy_list:\n","      for num in num_quarter_list:\n","        text = text.replace(quy + str(num),quy + \" \" + str(num))\n","    word_segment = rdrsegmenter.word_segment(text)\n","    res_text = tokenizer.texts_to_sequences(word_segment)\n","    res_text = pad_sequences(res_text, maxlen=100, padding='post')\n","    res_text = predict(model_ner, tag_tokenizer, res_text)\n","    # print(res_text)\n","    dict_ = match_pair_ner(word_segment[0], res_text[0])\n","    return dict_"],"metadata":{"id":"R4y-hDOGC6fM","executionInfo":{"status":"ok","timestamp":1693845993211,"user_tz":-420,"elapsed":327,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"00688950422503571962"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["**gradio**"],"metadata":{"id":"jo32F6KJErWc"}},{"cell_type":"code","source":["!pip install gradio"],"metadata":{"id":"bsRt8nAQEs1C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gradio as gr\n","\n","demo = gr.Interface(fn=infer_ner, inputs=\"text\", outputs=\"text\")\n","\n","if __name__ == \"__main__\":\n","    demo.launch(share=True,debug=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":750},"id":"P4-fJQr0EyZs","executionInfo":{"status":"ok","timestamp":1693846216753,"user_tz":-420,"elapsed":142869,"user":{"displayName":"Phú Nguyễn Đắc Hoàng","userId":"00688950422503571962"}},"outputId":"930861d5-1bac-4f5e-9457-46b2bd5b48fb"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","Running on public URL: https://cad34f47d6de6777ba.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://cad34f47d6de6777ba.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 4s 4s/step\n","1/1 [==============================] - 0s 119ms/step\n","1/1 [==============================] - 0s 117ms/step\n","1/1 [==============================] - 0s 119ms/step\n","1/1 [==============================] - 0s 213ms/step\n","1/1 [==============================] - 0s 117ms/step\n","Keyboard interruption in main thread... closing server.\n","Killing tunnel 127.0.0.1:7860 <> https://cad34f47d6de6777ba.gradio.live\n"]}]}]}